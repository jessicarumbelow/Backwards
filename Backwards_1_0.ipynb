{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jessicamarycooper/Backwards/blob/main/Backwards_1_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ar1aNn7TxI8r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86d6a73c-7a75-42e6-ac3c-bd4f5fea795e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: update in /usr/local/lib/python3.8/dist-packages (0.0.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: style==1.1.0 in /usr/local/lib/python3.8/dist-packages (from update) (1.1.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install update transformers\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, utils\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "utils.logging.set_verbosity_error()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "vocab_len= 50257\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side='left')\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\",pad_token_id=tokenizer.eos_token_id, vocab_size=vocab_len).to(device)\n",
        "model.eval()\n",
        "# the model will be in evaluation, not training, mode throughout\n",
        "word_embeddings = model.transformer.wte.weight.to(device)   \n",
        "# 'word_embeddings' tensor gives emeddings for each token in the vocab for this model,\n",
        "# has shape (vocab_len, embedding_dimension) which in this case = (50257, 768)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCgMUuO_33Wp"
      },
      "outputs": [],
      "source": [
        "def normalise(x, min_max=[]):     \n",
        "# normalises values of (array or tensor) x according to first (min) and second (max) values in list min_max. \n",
        "# This effectively defaults to [0,1] if the list doesn't contain exactly two elements. \n",
        "# The original code threw an error if min_max had length 1, so it's been changed slightly.\n",
        "\n",
        "# First normalise x to [0,1]\n",
        "    rnge = x.max() - x.min()\n",
        "    if rnge > 0:\n",
        "        x = (x - x.min())/rnge\n",
        "\n",
        "# Now, if there's a min and max given in min_max list, multiply by difference and add minimum\n",
        "    if len(min_max) > 1:\n",
        "        rnge = min_max[1] - min_max[0]\n",
        "        x = x * rnge + min_max[0]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def closest_tokens(emb, n=1):      \n",
        "# This finds the n tokens in the vocabulary that are closest in the embedding space (in terms of Euclidean distance) to a given word embedding (‘emb’).\n",
        "# Note that here 'emb' may or may not correspond to a token (i.e., it may or may not be a 'legal' embedding).\n",
        "# Function returns a 4-tuple (list of the n tokens, list of their indices, list of their distances from emb, and list of their embedding vectors)\n",
        "    torch.cuda.empty_cache()\n",
        "    dists = torch.linalg.norm(word_embeddings - emb, dim=1)\n",
        "    sorted_dists, ix = torch.sort(dists)\t \n",
        "    # sorted_dists is a list of all embedding distances from 'emb', across entire vocab, sorted in increasing order, \n",
        "    # ix is a list of their corresponding 'vocab indices'\n",
        "    tokens = [tokenizer.decode(i) for i in ix[:n]]\n",
        "    # For each of the first n 'vocab indices' in ix, we decode it into the string version of the corresponding token. \n",
        "    # These strings then constitute the list 'tokens'.\n",
        "    ixs = ix[:n]\n",
        "    dists = sorted_dists[:n]\n",
        "    embs = word_embeddings[ixs]  # Each of these n 'embeddings' is a tensor of shape (768,)\n",
        "    return tokens, ixs, dists, embs  \n",
        "\n",
        "\n",
        "def model_emb(inputs_embeds, output_len):\n",
        "# 'input_embeds' is a tensor of shape (batch_size, input_len, embedding_dim)\n",
        "# 'output_len' is an integer specifying the number of output tokens to generate\n",
        "# Note that this function doesn't involve a target output. It simply takes a tensor of input embeddings (based on input length),\n",
        "# calculates perplexities for that batch of input sequences,\n",
        "# and runs the batch of input sequences through GPT2, for each finding next tokens iteratively 'output_len' number of times\n",
        "    embs = inputs_embeds   # This is going to get expanded using 'output_embs'\n",
        "    logits = []\n",
        "    ixs = []\n",
        "    input_logits = None\n",
        "    for i in range(output_len):\n",
        "        model_out = model(inputs_embeds=embs, return_dict=True)\n",
        "        # Does a forward pass of GPT2 (or whichever model) on a batch of inputs (given as a tensor 'embs' of embeddings).\n",
        "        # This 'embs' will expand along its 1st dimension with each iteration.\n",
        "        # Outputs logits and more (hidden states, attention, etc.) as a dictionary 'model_out'.\n",
        "        # But we'll only be concerned with model_out.logits.\n",
        "\n",
        "        if i == 0:\n",
        "            input_logits = model_out.logits \n",
        "            # On first pass through loop, we simply use the logits of the model output\n",
        "            # That's a tensor of shape (batch_size, input_len, vocab_size) giving logits for each input in each batch.\n",
        "            # Presumably for each input, this is conditioned on the inputs that preceded it?\n",
        "\n",
        "        # On every pass throught the loop (including the first), we defined this tensor of shape (batch_size, 1, vocab_size):\n",
        "        last_logits = model_out.logits[:,-1].unsqueeze(1)  \n",
        "        # model_out.logits[:,-1] will be a 2D tensor of shape (batch_size, vocab_size), just giving logits for last input/embedding across all batches/tokens\n",
        "        # unsqueezing, we get tensor of shape (batch_size, 1, vocab_size) also giving logits of last input/embedding, differently formatted  \n",
        "        logits.append(last_logits)  # appends last_logits tensor to the 'logits' list \n",
        "        ix = torch.argmax(last_logits, dim=-1)  # for each batch, finds the vocab index of the token with the largest logit in last_logits\n",
        "        ixs.append(ix) # ...and appends this tensor of shape (batch_size,) (containing indices) it to the list 'ixs'\n",
        "        output_embs = word_embeddings[ix]   # for each batch, finds embedding for the token with that index...\n",
        "        embs = torch.cat([embs, output_embs], dim=1)  #...concatenates that tensor of embeddings to the 'embs' tensor in the first dimension before next iteration\n",
        "\n",
        "     # When the loop is completed 'embs' will be a tensor containing all of the input and output word embeddings produced by the model   \n",
        "     # ...so presumably of shape (batch_size, input_len + output_len, embedding_dim)\n",
        "\n",
        "    logits = torch.cat(logits, dim=1)   # this converts logits from a list of tensors to a single tensor, by concatenating all of the tensors in the list\n",
        "                                        # it will have shape (batch_size, output_len, vocab_size)\n",
        "    perp = perplexity(input_logits)     # 'input_logits' was calculated on first pass through loop where only input embeddings were involved\n",
        "    return logits, embs, perp          \n",
        "    # logits has shape (batch_size, output_len, vocab_size),         CHECK THAT!\n",
        "    # embs has shape (batch_size, input_len + output_len, embedding_dim)\n",
        "    # perp has shape (batch_size,)\n",
        "\n",
        "\n",
        "def perplexity(logits):\n",
        "    # logits is of shape (batch_size, 'sequence length', vocab_size)\n",
        "    # for all current calls, 'sequence length' is going to be input_len\n",
        "    probs, ix = torch.max(torch.softmax(logits, dim=-1), dim=-1)\n",
        "    # torch.softmax(logits, dim=-1) will also be a tensor of shape (batch_size, 'sequence length', vocab_size), \n",
        "    # but where the logits in the last dimension get converted into probabilities via softmax. torch.max() then pull out the largest of these and its index\n",
        "    # probs is a tensor that contains the maximum probability for each token in the embedding sequence, shape (batch_size, 'sequence length')\n",
        "    # ix is a tensor that contains the corresponding indices, also with shape (batch_size, 'sequence length')\n",
        "    perp = 1/ (torch.prod(probs, dim=-1)**(1/probs.shape[-1])) - 1\n",
        "    # defines a scalar that's larger with greater uncertainty (so if the probs are small, their product is small, the reciprocal of some power is large)\n",
        "    # probs.shape[-1] is output_len; the idea of raising the probs product to power 1/output_len is to make perplexities comparable across different output lengths\n",
        "    return perp\n",
        "\n",
        "\n",
        "# Here's the key function that optimises for a sequence of input embeddings, given a target_output string:\n",
        "def optimise_input(epochs=100, \n",
        "                   lr=0.1, \n",
        "                   rand_after=False,    # Do we re-initialise inputs tensor with random entries when an optimal input is found?\n",
        "                   w_freq=10,           # logging (write) frequency\n",
        "                   base_input=None,      # If none, start_inputs will be entirely random; \n",
        "                                         # otherwise it will be built by stacking this tensor and then gently \"noising\" all but the first copies\n",
        "                   batch_size=1, \n",
        "                   input_len=1, \n",
        "                   target_output=tokenizer.eos_token,    # Default target output is the \"end-of-string\" token; this won't generally be used\n",
        "                   output_len=None,\n",
        "                   dist_reg=1,       # distance regularisation coefficient\n",
        "                   perp_reg=0,       # perplexity regularisation coefficient; setting to 0 means perplexity loss isn't a thing\n",
        "                   plt_loss=False,   # Do we plot loss?\n",
        "                   loss_type='log_prob_loss', \n",
        "                   seed=0,\n",
        "                   return_early=True,    # finishes if single optimised input is found\n",
        "                   verbose=0,            # Controls how much info gets logged.\n",
        "                   lr_decay=False,       # Use learning rate decay? If so, a scheduler gets invoked.\n",
        "                   noise_coeff = 0.01):     # Introduced for generality in the construction of start_input[1:] below.\n",
        "    torch.manual_seed(seed)               # sets up PyTorch random number generator\n",
        "\n",
        "    if plt_loss:\n",
        "        plt.rcParams.update({'figure.figsize': (40,6)})\n",
        "\n",
        "    total_losses = []\n",
        "    losses = []\n",
        "    dists = []\n",
        "    perps = []\n",
        "    optimised_inputs = set()\n",
        "    done = None\n",
        "\n",
        "    output_ix = tokenizer.encode(target_output, return_tensors='pt')[0].to(device)\n",
        "    # output_ix is a 1-D tensor of shape (output_len,) that contains the indices of the tokens in the encoding of the string 'target_output'\n",
        "    # tokenizer.encode(target_output, return_tensors='pt') is a list containing this one tensor, hence the need for the [0]\n",
        "    # \"return_tensors='pt'\" ensures that we get a tensor in PyTorch format\n",
        "\n",
        "    if output_len == None or output_len < output_ix.shape[0]:                    # This won't generally be the case, but if we don't specify output_len (i.e. it's == None), then...\n",
        "        output_len = output_ix.shape[0]       # ...it will be set to the number of tokens in the encoding of the string 'target_output'\n",
        "    # Why not just set output_len = output_ix.shape[0] in all cases?\n",
        "    # Will there be situations where we want output_len to be of a different size to the number of tokens in target_output?\n",
        "\n",
        "    print('Optimising input of length {} to maximise output logits for \"{}\"'.format(input_len, target_output))\n",
        "    # Typically this would print something like 'Optimising input of length 6 to maximise output logits for \"KILL ALL HUMANS!\"'.\n",
        "\n",
        "    if base_input == None:\n",
        "        start_input = torch.rand(batch_size, input_len, word_embeddings.shape[-1]).to(device)\n",
        "        # If no base_input is provided, we construct start_input as a random tensor \n",
        "        # of shape (batch_size, input_len, embedding_dim) (embedding_dim = 768 for this GPT-2 model).\n",
        "        start_input = normalise(start_input,[word_embeddings.min(dim=0)[0], word_embeddings.max(dim=0)[0]])\n",
        "        # We normalise this random tensor so that its minimum and maximum values correspond to those in the entire word_embeddings tensor\n",
        "        # This dispenses with whole swathes of \"input space\" which contain no legal token embeddings \n",
        "        # (we're limiting ourselves to a kind of \"hull\" defined by the 50527 vocab tokens in the embedding space), \n",
        "        # which is a sensible place to look for optimised inputs.\n",
        "    else:\n",
        "        start_input = base_input.repeat(batch_size, 1, 1)\n",
        "        # If a base_input was given, it should be of shape (input_len, embedding_dim), \n",
        "        # and we build the start_input tensor by stacking 'batch_size' number of copies of this together...\n",
        "\n",
        "        if batch_size > 1:\n",
        "            start_input[1:] += (torch.rand_like(start_input[1:]) + torch.full_like(start_input[1:], -0.5)) * noise_coeff\n",
        "        #...and if we have more than one element in our batch, we \"noise\" the rest. \n",
        "        # This was originally done using \"*=\" (multiplying entries by small random numbers)\n",
        "        # We've changed this to \"+=\" (adding  small random numbers instead of multiplying by them).\n",
        "        # The original code would have pushed everything in a positive direction, hence the use of a tensor full of -0.5's.       \n",
        "\n",
        "    \n",
        "    input = torch.nn.Parameter(start_input, requires_grad=True)\n",
        "    # input is not a tensor, it's a Parameter object that wraps a tensor and adds additional functionality. \n",
        "    # 'input.data' is used below\n",
        "    \n",
        "    optimiser = torch.optim.Adam([input], lr=lr)\n",
        "    # standard optimiser; note that it generally operates on a list of tensors, so we're giving it a list of one tensor; standard learning rate\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', patience=20, cooldown=20, factor=0.5)\n",
        "    # this is used when loss hasn't improved for 20 timesteps; this scheduler will reduce the lr by a 'factor' of 0.5 when the \n",
        "    # validation loss stops improving for 'patience' (here 20) epochs, and will wait 'cooldown' (here 20) epochs before resuming normal operation.\n",
        "\n",
        "    # now we loop across training epochs\n",
        "    for e in range(epochs):\n",
        "\n",
        "        logits, emb, perp = model_emb(torch.clamp(input, word_embeddings.min(), word_embeddings.max()), output_len)\n",
        "        # Does forward pass on a 'clamped' version of the 'input' tensor (done to contain it within the 'hull' of the vocabulary within 'input space').\n",
        "        # Iterates to produce an output of output_len tokens, \n",
        "        # returns: 'logits' = tensor of logits for output, of shape (batch_size, output_len, vocab_size)\n",
        "        # 'emb': tensor of embeddings for input+output of shape (batch_size, input_len + output_len, embedding_dim); \n",
        "        # 'perp': the input sequence perplexities tensor, of shape (batch_size,)\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        # For each batch, output, converts the sequence of logits (of length 'vocab_size') in the 'logits' tensor to probabilities, using softmax\n",
        "\n",
        "        logits = (logits - logits.min(dim=-1)[0].unsqueeze(-1)) / (logits.max(dim=-1)[0].unsqueeze(-1) - logits.min(dim=-1)[0].unsqueeze(-1))\n",
        "        # This appears to be normalising the logits for each batch/output embedding so they're all between 0 and 1... \n",
        "        # This is for ease of visualisation.\n",
        "\n",
        "        perp_loss = perp.mean() * perp_reg\n",
        "        # That's taking the mean perp value across all batches, then regularising it. Currently perp_reg is set to 0, so perp_loss = 0.\n",
        "\n",
        "        if output_len > output_ix.shape[0]:\n",
        "            target_logits = torch.stack([logits[:, :, ix] for ix in output_ix], dim=-1)\n",
        "            target_logits = torch.max(target_logits, dim=-1)[0]\n",
        "            # logits is shape (batch_size, output_len, vocab_size) \n",
        "            # We throw out everything in the final dimension except those logits corresponding to indices of tokens in the target_ouput\n",
        "            # This gives tensor with shape (batch_size, output_len, output_ix.shape[0])\n",
        "            # We then take the maximum of those for each batch, output; this gives shape (batch_size, output_len)\n",
        "            # The [0] returns just the max (torch.max returns max, indices tuple)\n",
        "            target_probs = torch.stack([probs[:, :, ix] for ix in output_ix], dim=-1)\n",
        "            target_probs = torch.max(target_probs, dim=-1)[0]\n",
        "            # This does the analogous thing for probs.\n",
        "\n",
        "        else:\n",
        "            target_logits = torch.stack([logits[:,i, ix] for i, ix in enumerate(output_ix)], dim=-1)\n",
        "            target_probs = torch.stack([probs[:,i, ix] for i, ix in enumerate(output_ix)], dim=-1)\n",
        "            # This handles case where output_len == output_ix.shape[0]\n",
        "            # target_logits now of shape (batch_size, output_len)\n",
        "            # output_len < output_ix.shape[0] was dealt with in line 133\n",
        "            \n",
        "        token_dist = torch.stack([torch.stack([closest_tokens(e)[2].squeeze(-1) for e in input[b]]) for b in range(batch_size)])\n",
        "        # As far as I can tell, this creates a tensor of shape (batch_size, input_len, 1) which gives distance to nearest\n",
        "        # legal token embedding for each input embedding in each batch\n",
        "        mean_token_dist = token_dist.mean() * dist_reg\n",
        "        # A single scalar value, taking mean across the batch and input embeddings? \n",
        "\n",
        "\n",
        "        # There are currently four loss types, many more could be introduced.\n",
        "        # log_prob_loss is the current default.\n",
        "        if loss_type == 'logit_loss':\n",
        "            loss = torch.mean(1-target_logits)\n",
        "        elif loss_type == 'log_prob_loss':\n",
        "            loss = -torch.log(target_probs).mean()\n",
        "        elif loss_type == 'prob_loss':\n",
        "            loss = 1-torch.mean(target_probs)\n",
        "        elif loss_type == 'CE':\n",
        "            loss = torch.nn.functional.cross_entropy(logits.swapaxes(-1,-2), output_ix.repeat(batch_size, 1))\n",
        "\n",
        "        else:\n",
        "            print(loss_type + 'is not implemented.')\n",
        "            return\n",
        "\n",
        "        total_loss = torch.stack([mean_token_dist, loss, perp_loss]).mean()\n",
        "        # This is this just (mean_token_dist + loss + perp_loss)/3 tensorised across batches, yes?\n",
        "\n",
        "        total_losses.append(total_loss.detach().cpu().data)\n",
        "        losses.append(loss.detach().cpu().data)\n",
        "        dists.append(mean_token_dist.detach().cpu().data)\n",
        "        perps.append(perp_loss.detach().cpu().data)\n",
        "        # these four lists were intialised above. We're appeneding to the list each epoch. All are scalars.\n",
        "\n",
        "        closest_ix = torch.stack([torch.stack([closest_tokens(e)[1] for e in b]) for b in input]).squeeze(-1)\n",
        "        # This is similar to above, but building a tensor of indices of nearest embeddings, rather than distances.\n",
        "        # Iterates over batches, and for each batch iterates over embeddings, giving tensor of shape (batch_size, input_len).\n",
        "\n",
        "        model_outs = model.generate(closest_ix, max_length = output_len+input_len)\n",
        "        # The 'closest_ix' tensor is passed as the initial input sequence to the model, \n",
        "        # and the max_length parameter specifies the maximum length of the total sequence to generate.\n",
        "        # The output sequence will be terminated either when the end-of-sequence token is generated \n",
        "        # or when the maximum length is reached, whichever occurs first.\n",
        "        # \n",
        "        # The output of the model.generate method will be a tuple containing the generated sequences and the model's internal states. \n",
        "        # The generated sequences will be stored in a tensor of shape (batch_size, output_len+input_len). \n",
        "        # Each element of the tensor will be a sequence of tokens with a length of at most output_len+input_len.\n",
        "        \n",
        "        for b in range(batch_size):\n",
        "        # iterate over batches  \n",
        "            if output_len > output_ix.shape[0]:\n",
        "                if target_output in tokenizer.decode(model_outs[b][input_len:]):\n",
        "                    done = tokenizer.decode(model_outs[b][:input_len])\n",
        "                    optimised_inputs.add(done)\n",
        "                # model_outs[b][input_len:], for a batch b, is only looking at the *output* embeddings \n",
        "                # we decode these as tokens... is the target_output a substring?\n",
        "                # if so, we print the target_output and the decoded string that contains it\n",
        "                # 'done' is the string version of the model's output for given input, we add this to set 'optimised_inputs'.\n",
        "\n",
        "                if rand_after:\n",
        "                    input.data[b] = torch.rand_like(input[b])\n",
        "                    # This will require new normalisation function.\n",
        "                    # The idea here seems to be randomly re-initialise the input tensor once we've found an optimised input,\n",
        "                    # input.data is the tensor version of the 'input' Parameter object. Current values, without gradient!\n",
        "                    # That's of shape (batch_size, input_len, embedding_dim)\n",
        "\n",
        "            if tokenizer.decode(model_outs[b][input_len:]) == target_output:\n",
        "                done = tokenizer.decode(model_outs[b][:input_len])\n",
        "                optimised_inputs.add(done)\n",
        "                # model_outs[b][input_len:], for a batch b, is only looking at the *output* embeddings \n",
        "                # we decode these as tokens... is the target_output equal to output string?\n",
        "                # Nothing printed in this case.\n",
        "                # 'done' is the string version of the model's output for given input, we add this to set 'optimised_inputs'.\n",
        "                if rand_after:\n",
        "                    input.data[b] = torch.rand_like(input[b])\n",
        "                    # Random re-initialisation (if 'rand_after' set to True)\n",
        "\n",
        "  \n",
        "        if ((e+1) % w_freq == 0) or done and return_early:\n",
        "            display.clear_output(wait=True)  \n",
        "        # Every w epochs we write to log, unless we have found an optimised input before that and 'return_early' == True. \n",
        "        # I'm still not entirely sure about the idea of 'return_early'.\n",
        "\n",
        "            if plt_loss:\n",
        "                plt.plot(range(len(total_losses)), total_losses, label='Total Loss', color='black')\n",
        "                plt.plot(range(len(losses)), losses, label='Output Loss')\n",
        "                plt.plot(range(len(dists)), dists, label='Emb Dist Loss')\n",
        "                plt.plot(range(len(perps)), perps, label='Perp Loss')\n",
        "                plt.yscale('log')\n",
        "                plt.legend()\n",
        "\n",
        "                plt.show()\n",
        "\n",
        "            print('Inputs found: ', optimised_inputs)\n",
        "            print('{}/{} Output Loss: {} Emb Dist Loss: {} Perp Loss: {} LR: {}'.format(e+1, epochs, loss, mean_token_dist, perp_loss, optimiser.param_groups[0]['lr']))\n",
        "            if verbose == 3:\n",
        "                print('Target Probs: {}\\nTarget Logits: {}\\nInput Dists: {}\\nInput Perplexity: {}\\n'.format(target_probs.detach().cpu().numpy(), target_logits.detach().cpu().numpy(), token_dist.detach().cpu().numpy(), perp.detach().reshape(-1).cpu().numpy()))\n",
        "            # Optimised inputs and additional information are printed as part of log\n",
        "\n",
        "            for b in range(batch_size):\n",
        "                if verbose > 0:\n",
        "                    if verbose == 2:\n",
        "                        print(b, repr(' Raw embeddings: {}'.format(''.join([closest_tokens(e)[0][0] for e in emb[b]]))))\n",
        "                        # Change name to clarify (output of model if we just put in raw embeddings)\n",
        "                        # prints batch number; closest_tokens(e)[0] is a list of tokens, closest_tokens(e)[0] is the first (closest) of these\n",
        "                        # these get joined with separator '' (SHOULDN'T THAT BE ' '?)  \n",
        "                    print(b, repr(' Closest embeddings: {}'.format(tokenizer.decode(model_outs[b]), '\\n')))\n",
        "                        # WON'T THIS give string decodings of the embeddings, rather than the embeddings themselves?\n",
        "                else:\n",
        "                    print(repr(tokenizer.decode(model_outs[b])), end=' ')\n",
        "                    # The least verbose printed output. The 'end' parameter is used to specify the end-of-line string that is appended to the output. \n",
        "                    # By default, this is a newline character, but in this case it has been set to a single space character, \n",
        "                    # so the output will be separated by spaces rather than newlines.\n",
        "\n",
        "            if done and return_early:\n",
        "                print('\\nOptimised Input: \"{}\"'.format(done))\n",
        "                return optimised_inputs\n",
        "                # we know optimised_inputs set contains a single element in this case\n",
        "            \n",
        "        optimiser.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimiser.step()\n",
        "        # I assume these three lines are standard NN optimisation stuff?\n",
        "\n",
        "        if lr_decay:\n",
        "            scheduler.step(total_loss)\n",
        "         # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', patience=20, cooldown=20, factor=0.5) gets used if lr_decay == True\n",
        "    \n",
        "    return optimised_inputs\n",
        "    # that's a set of strings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tev0cDbkdbxO",
        "outputId": "45b23d11-a2b5-4e46-d592-5f8f0076390b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[767, 767, 767]\n",
            "[' 7', ' 7', ' 7']\n",
            "3\n",
            " 7 7 7 7 7 7 7\n"
          ]
        }
      ],
      "source": [
        "ix = tokenizer.encode(\" 7 7 7\")\n",
        "# list of 'vocab indices'\n",
        "print(ix)\n",
        "print([tokenizer.decode(i) for i in ix])\n",
        "# prints reconstruction of input string\n",
        "print(len(ix))\n",
        "# prints number of tokens\n",
        "output_len=4\n",
        "model_out = model.generate(torch.tensor(ix).unsqueeze(0).to(device), max_length = output_len + len(ix))\n",
        "print(tokenizer.decode(model_out[0]))\n",
        "# pushes input string throught GPT2 (or whichever model) iteratively producing output_len number of tokens, then prints input + output."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "target_output = \" the best player?\"\n",
        "input_len = 2\n",
        "\n",
        "base_input = word_embeddings[tokenizer.encode(target_output)].mean(dim=0)\n",
        "base_input = base_input.repeat(1, input_len, 1)\n",
        "\n",
        "tic = time()\n",
        "oi = optimise_input(base_input=base_input, \n",
        "                    plt_loss=False,\n",
        "                    verbose=2, \n",
        "                    epochs=500, \n",
        "                    lr_decay=False,\n",
        "                    return_early=False, \n",
        "                    lr=0.1, \n",
        "                    batch_size=20, \n",
        "                    target_output=target_output, \n",
        "                    output_len=4,\n",
        "                    input_len=input_len, \n",
        "                    w_freq=20, \n",
        "                    dist_reg=1, \n",
        "                    perp_reg=0,\n",
        "                    loss_type='log_prob_loss',\n",
        "                    noise_coeff = 0.75)\n",
        "toc = time()\n",
        "tt = toc - tic\n",
        "print('Time Taken: ', tt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "81r4gT4m2NQ1",
        "outputId": "3ebaad31-0828-43b3-fb29-5c8c9d0c39ae"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs found:  set()\n",
            "300/500 Output Loss: 2.0041980743408203 Emb Dist Loss: 5.514309883117676 Perp Loss: 0.0 LR: 0.1\n",
            "0 ' Raw embeddings:  the an the best,\\n'\n",
            "0 ' Closest embeddings:  the ancients, and the'\n",
            "1 ' Raw embeddings:  play bruised the best player?'\n",
            "1 ' Closest embeddings:  play bruised and battered.\\n'\n",
            "2 ' Raw embeddings:  80<|endoftext|> the best best.'\n",
            "2 ' Closest embeddings:  80<|endoftext|>The U.S'\n",
            "3 ' Raw embeddings:  Main those the best player?'\n",
            "3 ' Closest embeddings:  Main those who have been in'\n",
            "4 ' Raw embeddings:  ( Continuing the best player?'\n",
            "4 ' Closest embeddings:  ( Continuing )\\n\\n('\n",
            "5 ' Raw embeddings:  1 objects the the the the'\n",
            "5 ' Closest embeddings:  1 objects.\\n\\nThe'\n",
            "6 ' Raw embeddings:  her Berlin the best best.'\n",
            "6 ' Closest embeddings:  her Berlin Wall.\\n\\n'\n",
            "7 ' Raw embeddings: Obviously jobs the best player?'\n",
            "7 ' Closest embeddings: Obviously jobs are not just for'\n",
            "8 ' Raw embeddings:  bestAmid the best team.'\n",
            "8 ' Closest embeddings:  bestAmid the chaos, the'\n",
            "9 ' Raw embeddings:  Cleveland if the best player?'\n",
            "9 \" Closest embeddings:  Cleveland if he's healthy.\"\n",
            "10 ' Raw embeddings: linedSummary the best player?'\n",
            "10 ' Closest embeddings: linedSummary = \"\\n\\n'\n",
            "11 ' Raw embeddings:  blocking has the the..'\n",
            "11 ' Closest embeddings:  blocking has been a problem for'\n",
            "12 \" Raw embeddings:  sensational s's team.\\n\"\n",
            "12 ' Closest embeddings:  sensational siren song.\\n'\n",
            "13 ' Raw embeddings:  glance family the best player?'\n",
            "13 ' Closest embeddings:  glance family.\\n\\n\"'\n",
            "14 ' Raw embeddings:  using game the best player?'\n",
            "14 ' Closest embeddings:  using game-play.\\n'\n",
            "15 ' Raw embeddings:  clothes( the best player?'\n",
            "15 ' Closest embeddings:  clothes(s) of the'\n",
            "16 ' Raw embeddings:  high at the best player.'\n",
            "16 ' Closest embeddings:  high at the time.\\n'\n",
            "17 ' Raw embeddings:  wielded getting the best player?'\n",
            "17 ' Closest embeddings:  wielded getting a new job.'\n",
            "18 ' Raw embeddings:  way Spot the best player?'\n",
            "18 \" Closest embeddings:  way Spotty's been doing\"\n",
            "19 ' Raw embeddings:  predictable best fact fact fact\\n'\n",
            "19 ' Closest embeddings:  predictable best-case scenario for'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-0c0c7ac0fa03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m oi = optimise_input(base_input=base_input, \n\u001b[0m\u001b[1;32m     10\u001b[0m                     \u001b[0mplt_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-18fcc9e7603d>\u001b[0m in \u001b[0;36moptimise_input\u001b[0;34m(epochs, lr, rand_after, w_freq, base_input, batch_size, input_len, target_output, output_len, dist_reg, perp_reg, plt_loss, loss_type, seed, return_early, verbose, lr_decay, noise_coeff)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# I assume these three lines are standard NN optimisation stuff?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    "
      ],
      "metadata": {
        "id": "2jZJMakszJ_U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}